{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0_10000_1577671998.6491628.png',\n",
       " '0_10001_1577671998.65016.png',\n",
       " '0_10002_1577671998.65016.png',\n",
       " '0_10003_1577671998.651159.png',\n",
       " '0_10004_1577671998.6521559.png',\n",
       " '0_10005_1577671998.6531546.png',\n",
       " '0_10006_1577671998.6541514.png',\n",
       " '0_10007_1577671998.6541514.png',\n",
       " '0_10008_1577671998.6551478.png',\n",
       " '0_10009_1577671998.656145.png',\n",
       " '0_10010_1577671998.6571581.png',\n",
       " '0_10011_1577671998.6571581.png',\n",
       " '0_10012_1577671998.658139.png',\n",
       " '0_10013_1577671998.659136.png',\n",
       " '0_10014_1577671998.659136.png',\n",
       " '0_10015_1577671998.6601334.png',\n",
       " '0_10016_1577671998.6611307.png',\n",
       " '0_10017_1577671998.6611307.png',\n",
       " '0_10018_1577671998.6621292.png',\n",
       " '0_10019_1577671998.6631255.png',\n",
       " '0_10020_1577671998.6641226.png',\n",
       " '0_10021_1577671998.6641226.png',\n",
       " '0_10022_1577671998.66512.png',\n",
       " '0_10023_1577671998.66512.png',\n",
       " '0_10024_1577671998.6661177.png',\n",
       " '0_10025_1577671998.6671147.png',\n",
       " '0_10026_1577671998.6671147.png',\n",
       " '0_10027_1577671998.6681118.png',\n",
       " '0_10028_1577671998.6691096.png',\n",
       " '0_10029_1577671998.67011.png',\n",
       " '0_10030_1577671998.67011.png',\n",
       " '0_10031_1577671998.6711066.png',\n",
       " '0_10032_1577671998.6721332.png',\n",
       " '0_10033_1577671998.6721332.png',\n",
       " '0_10034_1577671998.6730998.png',\n",
       " '0_10035_1577671998.6740978.png',\n",
       " '0_10036_1577671998.6740978.png',\n",
       " '0_10037_1577671998.6750946.png',\n",
       " '0_10038_1577671998.6760936.png',\n",
       " '0_10039_1577671998.6760936.png',\n",
       " '0_10040_1577671998.6770883.png',\n",
       " '0_10041_1577671998.6780863.png',\n",
       " '0_10042_1577671998.6790838.png',\n",
       " '0_10043_1577671998.6790838.png',\n",
       " '0_10044_1577671998.6800802.png',\n",
       " '0_10045_1577671998.6810915.png',\n",
       " '0_10046_1577671998.682076.png',\n",
       " '0_10047_1577671998.6830733.png',\n",
       " '0_10048_1577671998.6830733.png',\n",
       " '0_10049_1577671998.684074.png',\n",
       " '0_10050_1577671998.6850698.png',\n",
       " '0_10051_1577671998.6860704.png',\n",
       " '0_10052_1577671998.6870646.png',\n",
       " '0_10053_1577671998.6880612.png',\n",
       " '0_10054_1577671998.6890578.png',\n",
       " '0_10055_1577671998.6900551.png',\n",
       " '0_10056_1577671998.6900551.png',\n",
       " '0_10057_1577671998.691052.png',\n",
       " '0_10058_1577671998.69205.png',\n",
       " '0_10059_1577671998.69205.png',\n",
       " '0_10060_1577671998.6930456.png',\n",
       " '0_10061_1577671998.694044.png',\n",
       " '0_10062_1577671998.6950414.png',\n",
       " '0_10063_1577671998.6950414.png',\n",
       " '0_10064_1577671998.6960478.png',\n",
       " '0_10065_1577671998.6970472.png',\n",
       " '0_10066_1577671998.6980336.png',\n",
       " '0_10067_1577671998.6980336.png',\n",
       " '0_10068_1577671998.6990306.png',\n",
       " '0_10069_1577671998.700028.png',\n",
       " '0_10070_1577671998.7010396.png',\n",
       " '0_10071_1577671998.7010396.png',\n",
       " '0_10072_1577671998.7020245.png',\n",
       " '0_10073_1577671998.704017.png',\n",
       " '0_10074_1577671998.7050138.png',\n",
       " '0_10075_1577671998.7050138.png',\n",
       " '0_10076_1577671998.7060113.png',\n",
       " '0_10077_1577671998.707008.png',\n",
       " '0_10078_1577671998.7080061.png',\n",
       " '0_10079_1577671998.7090046.png',\n",
       " '0_10080_1577671998.7090046.png',\n",
       " '0_10081_1577671998.7100003.png',\n",
       " '0_10082_1577671998.7109976.png',\n",
       " '0_10083_1577671998.711997.png',\n",
       " '0_10084_1577671998.7129922.png',\n",
       " '0_10085_1577671998.7139902.png',\n",
       " '0_10086_1577671998.7149887.png',\n",
       " '0_10087_1577671998.7159863.png',\n",
       " '0_10088_1577671998.7159863.png',\n",
       " '0_10089_1577671998.7179794.png',\n",
       " '0_10090_1577671998.7199738.png',\n",
       " '0_10091_1577671998.7209716.png',\n",
       " '0_10092_1577671998.7219687.png',\n",
       " '0_10093_1577671998.722966.png',\n",
       " '0_10094_1577671998.7239628.png',\n",
       " '0_10095_1577671998.724961.png',\n",
       " '0_10096_1577671998.7259579.png',\n",
       " '0_10097_1577671998.726968.png',\n",
       " '0_10098_1577671998.7279527.png',\n",
       " '0_10099_1577671998.72895.png',\n",
       " '0_10100_1577671998.7309444.png',\n",
       " '0_10101_1577671998.7319417.png',\n",
       " '0_10102_1577671998.7319417.png',\n",
       " '0_10103_1577671998.732942.png',\n",
       " '0_10104_1577671998.7349348.png',\n",
       " '0_10105_1577671998.7349348.png',\n",
       " '0_10106_1577671998.7359316.png',\n",
       " '0_10107_1577671998.737929.png',\n",
       " '0_10108_1577671998.7389255.png',\n",
       " '0_10109_1577671998.7389255.png',\n",
       " '0_10110_1577671998.7399228.png',\n",
       " '0_10111_1577671998.74092.png',\n",
       " '0_10112_1577671998.7419224.png',\n",
       " '0_10113_1577671998.7429132.png',\n",
       " '0_10114_1577671998.7439117.png',\n",
       " '0_10115_1577671998.744909.png',\n",
       " '0_10116_1577671998.744909.png',\n",
       " '0_10117_1577671998.745906.png',\n",
       " '0_10118_1577671998.746904.png',\n",
       " '0_10119_1577671998.746904.png',\n",
       " '0_10120_1577671998.7479012.png',\n",
       " '0_10121_1577671998.748898.png',\n",
       " '0_10122_1577671998.7499025.png',\n",
       " '0_10123_1577671998.750893.png',\n",
       " '0_10124_1577671998.7518904.png',\n",
       " '0_10125_1577671998.7529.png',\n",
       " '0_10126_1577671998.7538846.png',\n",
       " '0_10127_1577671998.754882.png',\n",
       " '0_10128_1577671998.755878.png',\n",
       " '0_10129_1577671998.756876.png',\n",
       " '0_10130_1577671998.7578733.png',\n",
       " '0_10131_1577671998.7588706.png',\n",
       " '0_10132_1577671998.7598677.png',\n",
       " '0_10133_1577671998.7608645.png',\n",
       " '0_10134_1577671998.7618625.png',\n",
       " '0_10135_1577671998.7628632.png',\n",
       " '0_10136_1577671998.7638564.png',\n",
       " '0_10137_1577671998.7648547.png',\n",
       " '0_10138_1577671998.7658515.png',\n",
       " '0_10139_1577671998.7668488.png',\n",
       " '0_9960_1577671998.6182477.png',\n",
       " '0_9961_1577671998.6192439.png',\n",
       " '0_9962_1577671998.6202426.png',\n",
       " '0_9963_1577671998.6212404.png',\n",
       " '0_9964_1577671998.6222363.png',\n",
       " '0_9965_1577671998.6232333.png',\n",
       " '0_9966_1577671998.6242306.png',\n",
       " '0_9967_1577671998.6252277.png',\n",
       " '0_9968_1577671998.6262257.png',\n",
       " '0_9969_1577671998.6262257.png',\n",
       " '0_9970_1577671998.6272216.png',\n",
       " '0_9971_1577671998.6282191.png',\n",
       " '0_9972_1577671998.6292186.png',\n",
       " '0_9973_1577671998.6302147.png',\n",
       " '0_9974_1577671998.6302147.png',\n",
       " '0_9975_1577671998.631212.png',\n",
       " '0_9976_1577671998.63221.png',\n",
       " '0_9977_1577671998.633207.png',\n",
       " '0_9978_1577671998.6342049.png',\n",
       " '0_9979_1577671998.6342049.png',\n",
       " '0_9980_1577671998.635208.png',\n",
       " '0_9981_1577671998.6361995.png',\n",
       " '0_9982_1577671998.6371973.png',\n",
       " '0_9983_1577671998.6381943.png',\n",
       " '0_9984_1577671998.6391895.png',\n",
       " '0_9985_1577671998.6401868.png',\n",
       " '0_9986_1577671998.6401868.png',\n",
       " '0_9987_1577671998.641184.png',\n",
       " '0_9988_1577671998.6421814.png',\n",
       " '0_9989_1577671998.6431787.png',\n",
       " '0_9990_1577671998.6431787.png',\n",
       " '0_9991_1577671998.6441758.png',\n",
       " '0_9992_1577671998.6441758.png',\n",
       " '0_9993_1577671998.6451735.png',\n",
       " '0_9994_1577671998.6461706.png',\n",
       " '0_9995_1577671998.6461706.png',\n",
       " '0_9996_1577671998.6471682.png',\n",
       " '0_9997_1577671998.6471682.png',\n",
       " '0_9998_1577671998.6481655.png',\n",
       " '0_9999_1577671998.6491628.png']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"C:/Users/SASIREKHA/Desktop/Traffic_sign_det/myData/0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = []\n",
    "Target = []\n",
    "for i in range(43):\n",
    "    for j in os.listdir(\"C:/Users/SASIREKHA/Desktop/Traffic_sign_det/myData\" + \"/\" + str(i)):\n",
    "        img = cv2.imread(\"C:/Users/SASIREKHA/Desktop/Traffic_sign_det/myData\" + \"/\" + str(i) +\"/\" + j)\n",
    "        Features.append(img)\n",
    "        Target.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " type(Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Features = np.array(Features)\n",
    "Target = np.array(Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34799, 32, 32, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34799,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Train_x, Test_x, Train_y, Test_y = train_test_split(Features,Target,test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27839, 32, 32, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 3072 into shape (32,32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-09d2dcc076a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTest_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 3072 into shape (32,32)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.imshow(Test_x[i].reshape(32,32))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(Test_x[0].reshape(32,32))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images=[]\n",
    "for i in range(len(Train_x)):\n",
    "    img=cv2.cvtColor(Train_x[i],cv2.COLOR_BGR2GRAY)\n",
    "    Images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(Images):\n",
    "    Images=Images/255\n",
    "    return Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_x = np.array(list(map(preprocessing, Train_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_x = np.array(Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27839, 32, 32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_x=Train_x.reshape(27839, 32, 32,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27839, 32, 32, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6960, 32, 32, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images=[]\n",
    "for i in range(len(Test_x)):\n",
    "    img=cv2.cvtColor(Test_x[i],cv2.COLOR_BGR2GRAY)\n",
    "    Images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(Images):\n",
    "    Images=Images/255\n",
    "    return Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_x = np.array(list(map(preprocessing, Test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_x=np.array(Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6960, 32, 32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_x=Test_x.reshape(6960, 32, 32,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27839, 32, 32, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6960, 32, 32, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_y=to_categorical(Train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27839, 43)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_y=to_categorical(Test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6960, 43)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Specify the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,Flatten,Conv2D,MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(60,(3,3),activation=\"relu\",input_shape=(32,32,1)))\n",
    "model.add(Conv2D(60,(3,3),activation=\"relu\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(60,(3,3),activation=\"relu\"))\n",
    "model.add(Conv2D(60,(3,3),activation=\"relu\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(600,activation=\"relu\"))\n",
    "model.add(Dense(43,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=0.001),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataGen=ImageDataGenerator(rotation_range=10,width_shift_range=0.1,height_shift_range=0.1,shear_range=0.1,zoom_range=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1392/1392 [==============================] - 125s 89ms/step - loss: 0.0903 - accuracy: 0.9815\n",
      "Epoch 2/20\n",
      "1392/1392 [==============================] - 121s 87ms/step - loss: 0.0947 - accuracy: 0.9801\n",
      "Epoch 3/20\n",
      "1392/1392 [==============================] - 118s 85ms/step - loss: 0.0963 - accuracy: 0.9806\n",
      "Epoch 4/20\n",
      "1392/1392 [==============================] - 119s 86ms/step - loss: 0.0939 - accuracy: 0.9801\n",
      "Epoch 5/20\n",
      "1392/1392 [==============================] - 121s 87ms/step - loss: 0.0811 - accuracy: 0.9813\n",
      "Epoch 6/20\n",
      "1392/1392 [==============================] - 119s 86ms/step - loss: 0.1006 - accuracy: 0.9804\n",
      "Epoch 7/20\n",
      "1392/1392 [==============================] - 120s 86ms/step - loss: 0.0774 - accuracy: 0.9835\n",
      "Epoch 8/20\n",
      "1392/1392 [==============================] - 128s 92ms/step - loss: 0.0843 - accuracy: 0.9824\n",
      "Epoch 9/20\n",
      "1392/1392 [==============================] - 124s 89ms/step - loss: 0.1066 - accuracy: 0.9797\n",
      "Epoch 10/20\n",
      " 703/1392 [==============>...............] - ETA: 1:03 - loss: 0.0794 - accuracy: 0.98"
     ]
    }
   ],
   "source": [
    "model.fit_generator(dataGen.flow(Train_x,Train_y,batch_size=20),epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json=model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"C:/Users/SASIREKHA/Desktop/Traffic_sign_det/AITraff.json\",\"w\")\n",
    "f.write(model_json)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"C:/Users/SASIREKHA/Desktop/Traffic_sign_det/AITrafficWeights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file=open(\"C:/Users/SASIREKHA/Desktop/Traffic_sign_det/AITraff.json\",\"r\")\n",
    "loaded_model_json=json_file.read()\n",
    "json_file.close()\n",
    "loaded_model=model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights(\"C:/Users/SASIREKHA/Desktop/Traffic_sign_det/AITrafficWeights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = loaded_model.predict(Test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.39819949e-08 3.93475641e-10 1.06582975e-05 ... 6.06951156e-11\n",
      "  1.91210298e-10 1.00780304e-11]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 2.17363804e-36 0.00000000e+00 ... 3.03406313e-35\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " ...\n",
      " [1.77782970e-22 1.27416864e-22 2.60037490e-26 ... 6.25722554e-28\n",
      "  2.71082020e-33 4.28003360e-28]\n",
      " [4.11936753e-22 3.88167189e-12 1.93145433e-09 ... 4.01900328e-14\n",
      "  1.19668887e-22 4.59729014e-23]\n",
      " [2.12802709e-11 2.85655267e-11 7.14879933e-09 ... 6.23696372e-11\n",
      "  4.44192345e-13 8.19736463e-15]]\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.39819949e-08 3.93475641e-10 1.06582975e-05 9.99900937e-01\n",
      " 4.29396824e-10 6.11171781e-05 1.59235583e-10 2.54358015e-12\n",
      " 2.20520269e-11 1.85055157e-07 2.68001695e-05 6.30923003e-09\n",
      " 7.36571359e-11 8.27963476e-10 1.22782076e-10 7.25420080e-08\n",
      " 2.37369524e-09 1.32405873e-13 5.76651782e-10 2.62432496e-17\n",
      " 3.12518079e-15 6.63897051e-13 5.14377874e-13 7.62052842e-15\n",
      " 5.73949779e-15 4.13246298e-10 1.82800819e-09 1.56691066e-12\n",
      " 2.89081381e-09 1.50890862e-11 1.59758968e-12 7.73339082e-08\n",
      " 4.15381878e-12 1.26336621e-08 3.70944192e-10 2.59376165e-09\n",
      " 1.75397405e-11 3.29091612e-15 1.79857821e-07 4.00364603e-11\n",
      " 6.06951156e-11 1.91210298e-10 1.00780304e-11]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99990094\n"
     ]
    }
   ],
   "source": [
    "print(np.amax(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl=np.argmax(a, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(predictions, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capt=cv2.VideoCapture(0)\n",
    "capt.set(3,640)\n",
    "capt.set(4,480)\n",
    "capt.set(10,180)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClassName(classNo):\n",
    "    if   classNo == 0: return 'Speed Limit 20 km/h'\n",
    "    elif classNo == 1: return 'Speed Limit 30 km/h'\n",
    "    elif classNo == 2: return 'Speed Limit 50 km/h'\n",
    "    elif classNo == 3: return 'Speed Limit 60 km/h'\n",
    "    elif classNo == 4: return 'Speed Limit 70 km/h'\n",
    "    elif classNo == 5: return 'Speed Limit 80 km/h'\n",
    "    elif classNo == 6: return 'End of Speed Limit 80 km/h'\n",
    "    elif classNo == 7: return 'Speed Limit 100 km/h'\n",
    "    elif classNo == 8: return 'Speed Limit 120 km/h'\n",
    "    elif classNo == 9: return 'No passing'\n",
    "    elif classNo == 10: return 'No passing for vechiles over 3.5 metric tons'\n",
    "    elif classNo == 11: return 'Right-of-way at the next intersection'\n",
    "    elif classNo == 12: return 'Priority road'\n",
    "    elif classNo == 13: return 'Yield'\n",
    "    elif classNo == 14: return 'Stop'\n",
    "    elif classNo == 15: return 'No vechiles'\n",
    "    elif classNo == 16: return 'Vechiles over 3.5 metric tons prohibited'\n",
    "    elif classNo == 17: return 'No entry'\n",
    "    elif classNo == 18: return 'General caution'\n",
    "    elif classNo == 19: return 'Dangerous curve to the left'\n",
    "    elif classNo == 20: return 'Dangerous curve to the right'\n",
    "    elif classNo == 21: return 'Double curve'\n",
    "    elif classNo == 22: return 'Bumpy road'\n",
    "    elif classNo == 23: return 'Slippery road'\n",
    "    elif classNo == 24: return 'Road narrows on the right'\n",
    "    elif classNo == 25: return 'Road work'\n",
    "    elif classNo == 26: return 'Traffic signals'\n",
    "    elif classNo == 27: return 'Pedestrians'\n",
    "    elif classNo == 28: return 'Children crossing'\n",
    "    elif classNo == 29: return 'Bicycles crossing'\n",
    "    elif classNo == 30: return 'Beware of ice/snow'\n",
    "    elif classNo == 31: return 'Wild animals crossing'\n",
    "    elif classNo == 32: return 'End of all speed and passing limits'\n",
    "    elif classNo == 33: return 'Turn right ahead'\n",
    "    elif classNo == 34: return 'Turn left ahead'\n",
    "    elif classNo == 35: return 'Ahead only'\n",
    "    elif classNo == 36: return 'Go straight or right'\n",
    "    elif classNo == 37: return 'Go straight or left'\n",
    "    elif classNo == 38: return 'Keep right'\n",
    "    elif classNo == 39: return 'Keep left'\n",
    "    elif classNo == 40: return 'Roundabout mandatory'\n",
    "    elif classNo == 41: return 'End of no passing'\n",
    "    elif classNo == 42: return 'End of no passing by vechiles over 3.5 metric tons'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(image):\n",
    "    image=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    image=image/255\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    message,frame=capt.read()\n",
    "    image=np.array(frame)\n",
    "    image=cv2.resize(image,(32,32))\n",
    "    image=preprocessing(image)\n",
    "    image=image.reshape(1,32,32,1)\n",
    "    predictions=loaded_model.predict(image)\n",
    "    classIndex=loaded_model.predict_classes(image)\n",
    "    cv2.putText(frame,\"Class: \",(20,35),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)\n",
    "    cv2.putText(frame,\"Probability: \",(20,75),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)\n",
    "    probabilityValue=np.amax(predictions)\n",
    "    if probabilityValue>0.75:\n",
    "        cv2.putText(frame,getClassName(classIndex),(120,35),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)\n",
    "        cv2.putText(frame,str(probabilityValue*100) + \"%\",(200,75),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)\n",
    "    cv2.imshow(\"Model Prediction\",frame)\n",
    "    ReceivedValue=cv2.waitKey(1)\n",
    "    if ReceivedValue==ord(\"s\"):\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00318384 0.02724909 0.01597013 0.07177821 0.03689392 0.08438193\n",
      "  0.01351297 0.0216626  0.05400373 0.0115045  0.01479458 0.00461614\n",
      "  0.00913021 0.06987138 0.00850072 0.09025374 0.00117385 0.00633813\n",
      "  0.17536777 0.00216815 0.01871557 0.00418117 0.00347474 0.00310328\n",
      "  0.00809683 0.01386078 0.0344771  0.00216951 0.00781268 0.00771448\n",
      "  0.00939501 0.02210635 0.04303994 0.00523957 0.00796606 0.00777499\n",
      "  0.01219304 0.00171726 0.04014466 0.00474759 0.01336548 0.0025496\n",
      "  0.00379866]]\n",
      "[18]\n",
      "0.17536777\n"
     ]
    }
   ],
   "source": [
    "image=cv2.imread(\"C:/Users/SASIREKHA/Desktop/Traffic_sign_det/data.png\")\n",
    "image=np.array(image)\n",
    "image=cv2.resize(image,(32,32))\n",
    "image=preprocessing(image)\n",
    "image=image.reshape(1,32,32,1)\n",
    "predictions=model.predict(image)\n",
    "classIndex=model.predict_classes(image)\n",
    "\n",
    "probabilityValue=np.amax(predictions)\n",
    "print(predictions)\n",
    "print(classIndex)\n",
    "print(probabilityValue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/218 [==============================] - 7s 33ms/step - loss: 0.0326 - accuracy: 0.9932\n",
      "[0.0325826033949852, 0.9932471513748169]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(Test_x,Test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
